{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlmTechSoftware/piper/blob/research/training_data/research/feynman_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOJFJG6WFQ09"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqjcihTACqSG"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrESAlTsxScr",
        "outputId": "5036017b-491a-46f0-d7fe-a04cdeb7638c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (23.2.1)\n",
            "Requirement already satisfied: opencv-python in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (4.8.0.76)\n",
            "Requirement already satisfied: scikit-image in /home/elal/.local/lib/python3.11/site-packages (0.21.0)\n",
            "Requirement already satisfied: python-dotenv in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (1.0.0)\n",
            "Requirement already satisfied: pycocotools in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (2.0.7)\n",
            "Requirement already satisfied: dataclasses-json in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (0.6.0)\n",
            "Requirement already satisfied: supervision in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (0.14.0)\n",
            "Requirement already satisfied: colored in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (2.2.3)\n",
            "Requirement already satisfied: wandb in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (0.15.11)\n",
            "Requirement already satisfied: torch in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (0.15.2)\n",
            "Requirement already satisfied: pandas in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (2.1.1)\n",
            "Requirement already satisfied: seaborn in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (0.12.2)\n",
            "Requirement already satisfied: torchviz in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (0.0.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /home/elal/.local/lib/python3.11/site-packages (from opencv-python) (1.25.1)\n",
            "Requirement already satisfied: scipy>=1.8 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from scikit-image) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from scikit-image) (3.0)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from scikit-image) (9.5.0)\n",
            "Requirement already satisfied: imageio>=2.27 in /home/elal/.local/lib/python3.11/site-packages (from scikit-image) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /home/elal/.local/lib/python3.11/site-packages (from scikit-image) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /home/elal/.local/lib/python3.11/site-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: packaging>=21 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from scikit-image) (23.0)\n",
            "Requirement already satisfied: lazy_loader>=0.2 in /home/elal/.local/lib/python3.11/site-packages (from scikit-image) (0.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pycocotools) (3.7.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/elal/.local/lib/python3.11/site-packages (from dataclasses-json) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/elal/.local/lib/python3.11/site-packages (from dataclasses-json) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from supervision) (4.8.0.76)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from supervision) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (3.1.32)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (2.28.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (1.30.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (65.5.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from wandb) (4.24.2)\n",
            "Requirement already satisfied: filelock in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (3.12.3)\n",
            "Requirement already satisfied: typing-extensions in /home/elal/.local/lib/python3.11/site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: jinja2 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: wheel in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.2)\n",
            "Requirement already satisfied: cmake in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: graphviz in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/elal/.local/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/elal/.local/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/elal/.local/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/elal/.local/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /home/elal/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
            "Dependencies installation complete.\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --upgrade pip\n",
        "!python3 -m pip install  opencv-python \\\n",
        "              scikit-image \\\n",
        "              python-dotenv \\\n",
        "              pycocotools \\\n",
        "              dataclasses-json \\\n",
        "              supervision \\\n",
        "              colored \\\n",
        "              wandb \\\n",
        "\t\t\t  torch \\\n",
        "              torchvision \\\n",
        "\t\t\t  pycocotools \\\n",
        "\t\t\t  pandas \\\n",
        "\t\t\t  seaborn \\\n",
        "              torchviz -U && echo \"Dependencies installation complete.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ENtKgHgx6RE"
      },
      "source": [
        "## WandB login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "s5ANw45px5V5",
        "outputId": "4539c106-8ea4-473e-c163-7351b3a33626"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwych\u001b[0m (\u001b[33malmtech\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmM5s_1FcHL"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OJlNPcXyxYI"
      },
      "source": [
        "## Dataset Handler Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gaRrGXg-F7CT"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "35hBUOfbzzvw"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "_tensor_pil_transform = T.ToPILImage()\n",
        "\n",
        "def tensor_to_pil(tensor):\n",
        "    return _tensor_pil_transform(tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np \n",
        "import os\n",
        "\n",
        "from typing import List, Tuple\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "class COCOInstanceDataset(Dataset):\n",
        "    def __init__(self, dataset_dir: str = \"dataset/train/\"):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.data = __class__.get_mask_image_pairs(dataset_dir)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_mask_image_pairs(directory: str) -> List[Tuple[str, str]]:\n",
        "        mask_image_pairs = [\n",
        "            (\n",
        "                os.path.join(directory, f\"{os.path.splitext(filename)[0]}_mask.png\"),\n",
        "                os.path.join(directory, filename)\n",
        "            )\n",
        "            for filename in os.listdir(directory)\n",
        "            if filename.endswith(\".jpg\") and os.path.isfile(os.path.join(directory, f\"{os.path.splitext(filename)[0]}_mask.png\"))\n",
        "        ]\n",
        "        return mask_image_pairs\n",
        "        \n",
        "    def __getitem__(self, idx: int):\n",
        "        img_file, mask_file = self.data[idx]\n",
        "        img = Image.open(img_file).convert(\"RGB\")\n",
        "        mask = Image.open(mask_file) \n",
        "\n",
        "        mask = np.array(mask)\n",
        "        obj_ids = np.unique(mask)\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        masks = np.zeros((num_objs , mask.shape[0] , mask.shape[1]))\n",
        "        for i in range(num_objs):\n",
        "            masks[i][mask == i+1] = True\n",
        "        boxes = []\n",
        "\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin , ymin , xmax , ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes , dtype = torch.float32)\n",
        "        labels = torch.ones((num_objs,) , dtype = torch.int64)\n",
        "        masks = torch.as_tensor(masks , dtype = torch.uint8)\n",
        "        \n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "\n",
        "        return T.ToTensor()(img) , target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = COCOInstanceDataset(dataset_dir=\"dataset/train/\")\n",
        "dataloader = DataLoader(dataset=dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iv4XD-6VDDzP",
        "outputId": "9e679c84-8246-4e99-da4d-21ef1e8b0883"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     imgs, masks \u001b[39m=\u001b[39m batch\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mimgs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mmasks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "\u001b[1;32m/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m masks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((num_objs , mask\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] , mask\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_objs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     masks[i][mask \u001b[39m==\u001b[39;49m i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m boxes \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_objs):\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "    imgs, masks = batch\n",
        "    print(f\"{imgs[0].shape} {masks[0].shape}\")\n",
        "    #display(tensor_to_pil(imgs[0] * 255))\n",
        "    #display(tensor_to_pil(masks[0] * 40))\n",
        "    display(tensor_to_pil(imgs[0]*100 + masks[0]*120))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCLnU8ZOCwlD"
      },
      "source": [
        "## FeynMAN Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K5bPDcMYC0Y9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FeynMAN(nn.Module):\n",
        "    def __init__(self, num_classes: int = 3):\n",
        "        super(__class__, self).__init__()\n",
        "\n",
        "        # Backbone (You can replace this with your custom backbone)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Region Proposal Network (RPN)\n",
        "        self.rpn_conv = nn.Conv2d(64, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.rpn_cls = nn.Conv2d(512, 2, kernel_size=1, stride=1)\n",
        "        self.rpn_reg = nn.Conv2d(512, 4, kernel_size=1, stride=1)\n",
        "\n",
        "        # RoI Heads\n",
        "        self.roi_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.fc1 = nn.Linear(7*7*512, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.cls_score = nn.Linear(1024, num_classes + 1)\n",
        "        self.bbox_pred = nn.Linear(1024, (num_classes + 1) * 4)\n",
        "\n",
        "        # Mask Heads\n",
        "        self.mask_conv1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.mask_conv2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.mask_deconv = nn.ConvTranspose2d(512, num_classes, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Backbone\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # RPN\n",
        "        rpn_feature = self.rpn_conv(x)\n",
        "        rpn_cls = self.rpn_cls(rpn_feature)\n",
        "        rpn_reg = self.rpn_reg(rpn_feature)\n",
        "\n",
        "        # RoI Heads\n",
        "        x = self.roi_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        cls_score = self.cls_score(x)\n",
        "        bbox_pred = self.bbox_pred(x)\n",
        "\n",
        "        # Mask Heads\n",
        "        mask_feature = self.mask_conv1(x)\n",
        "        mask_feature = self.mask_conv2(mask_feature)\n",
        "        mask = self.mask_deconv(mask_feature)\n",
        "\n",
        "        return rpn_cls, rpn_reg, cls_score, bbox_pred, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Cxaa7VFNbW"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af1ovG1ZFPmu",
        "outputId": "e2c23ebb-ab34-48ea-f197-9902504b277c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Sep 23 12:37:56 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1660 ...    Off | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   45C    P5              20W / 140W |    399MiB /  6144MiB |     22%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      1483      G   /usr/lib/Xorg                               177MiB |\n",
            "|    0   N/A  N/A      1901      G   picom                                        30MiB |\n",
            "|    0   N/A  N/A      4744      G   ...,WinRetrieveSuggestionsOnlyOnDemand       43MiB |\n",
            "|    0   N/A  N/A   2680753      G   ...sion,SpareRendererForSitePerProcess      124MiB |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "YES YOU CAN TRAIN WITH CUDA!\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi && echo \"YES YOU CAN TRAIN WITH CUDA!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MiG5glMSF3CF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    epochs: int = EPOCHS,\n",
        "    learning_rate: float = LEARNING_RATE,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "):\n",
        "    # Watch the model for cool weights & biases\n",
        "    wandb.watch(model)\n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss() # nn.BCELoss() #### TODO: change?\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    params = model.parameters()\n",
        "    optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=learning_rate/10)\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_epoch_loss = 0\n",
        "        val_epoch_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, dt in enumerate(dataloader):\n",
        "            imgs = [dt[0][0].to(device) , dt[1][0].to(device)]\n",
        "\n",
        "            targ = [dt[0][1], dt[1][1]]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targ]\n",
        "\n",
        "            loss = model(imgs, targets)\n",
        "            losses = sum([l for l in loss.values()])\n",
        "\n",
        "            train_epoch_loss += losses.cpu().detach().numpy()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print(\n",
        "                f\" # Epoch [{epoch+1}/{epochs}] Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss:.4f}\"\n",
        "            )\n",
        "            \n",
        "            # Log to WandB\n",
        "            wandb.log({\"loss\": loss})\n",
        "            wandb.log({\"train_epoch_loss\": train_epoch_loss})\n",
        "            wandb.log({\"val_epoch_loss\": val_epoch_loss})\n",
        "\n",
        "    # for epoch in range(epochs):\n",
        "    #     model.train()\n",
        "    #     running_loss = 0.0\n",
        "\n",
        "    #     for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "    #         # Convert to the GPU or other devicetype\n",
        "    #         images, labels = images.to(device).float(), labels.to(device)\n",
        "\n",
        "    #         optimizer.zero_grad()\n",
        "\n",
        "    #         # Forward pass\n",
        "    #         rpn_cls, rpn_reg, cls_score, bbox_pred, mask = model(images)\n",
        "\n",
        "    #         # Define your loss calculation based on your problem\n",
        "    #         # For example, if you're doing object detection, you'd need to define the losses for rpn_cls, rpn_reg, cls_score, and bbox_pred\n",
        "    #         # Similarly, for a segmentation task, you'd need to define the loss for the mask\n",
        "\n",
        "    #         loss = your_loss_function(rpn_cls, rpn_reg, cls_score, bbox_pred, mask, labels)\n",
        "    #         wandb.log({\"Loss\": loss})\n",
        "\n",
        "    #         # Backward and optimize\n",
        "    #         loss.backward()\n",
        "    #         optimizer.step()\n",
        "\n",
        "    #         running_loss += loss.item()\n",
        "\n",
        "    #         print(\n",
        "    #             f\" # Epoch [{epoch+1}/{epochs}] Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss:.4f}\"\n",
        "    #         )\n",
        "\n",
        "    #         del images, labels\n",
        "\n",
        "    #     # Log the running loss\n",
        "    #     wandb.log({\"Running loss\": running_loss})\n",
        "\n",
        "\n",
        "    # print(\"BEGIN TRAINING!\")\n",
        "    # for epoch in range(epochs):\n",
        "    #     model.train()\n",
        "    #     running_loss = 0.0\n",
        "    #     print(epoch)\n",
        "\n",
        "    #     for batch_idx, (images, masks) in enumerate(dataloader):\n",
        "    #         images, masks = images.to(device).float(), masks.to(device).float()\n",
        "    #         print(\"Running batch: \", batch_idx)\n",
        "            \n",
        "    #         # Forward pass\n",
        "    #         print(\" - Forward pass\")\n",
        "    #         outputs = model(images)\n",
        "    #         # preds = torch.argmax(outputs, dim=1).float()\n",
        "    #         print(\" - Loss calc\")\n",
        "    #         loss = criterion(outputs, masks)\n",
        "\n",
        "    #         # Backpropagation and optimization\n",
        "    #         print(\" - Zero grad\")\n",
        "    #         optimizer.zero_grad()\n",
        "    #         print(\" - Backward prop\")\n",
        "    #         loss.backward()\n",
        "    #         print(\" - Optim step\")\n",
        "    #         optimizer.step()\n",
        "\n",
        "    #         running_loss += loss.item()\n",
        "    #         wandb.log({\"Loss\": loss})\n",
        "\n",
        "\n",
        "    #         del images, masks\n",
        "\n",
        "    #     wandb.log({\"Running loss\": running_loss})\n",
        "\n",
        "    print(\"END TRAINING!\")\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRs5JTpcWrnv"
      },
      "source": [
        "### Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nQKN7f_HWtlO"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/elal/Projects/almtech/aidetic/piper/training_data/research/wandb/run-20230923_004221-8j26nmue</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/almtech/feynman_0/runs/8j26nmue' target=\"_blank\">proud-violet-13</a></strong> to <a href='https://wandb.ai/almtech/feynman_0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/almtech/feynman_0' target=\"_blank\">https://wandb.ai/almtech/feynman_0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/almtech/feynman_0/runs/8j26nmue' target=\"_blank\">https://wandb.ai/almtech/feynman_0/runs/8j26nmue</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = FeynMAN()\n",
        "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#device_type = \"cpu\"\n",
        "device = torch.device(device_type)\n",
        "\n",
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "\t# set the wandb project where this run will be logged\n",
        "\tproject=\"feynman_0\",\n",
        "\n",
        "\t# track hyperparameters and run metadata\n",
        "\tconfig={\n",
        "\t\"learning_rate\": LEARNING_RATE,\n",
        "\t\"epochs\": EPOCHS,\n",
        "\t\"batch_size\": BATCH_SIZE,\n",
        "\t\"architecture\": \"FeynMAN\",\n",
        "\t\"dataset\": \"feynman_v5i\",\n",
        "\t}\n",
        ")\n",
        "\n",
        "# Put the model on the GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBL0_8NEIbg2"
      },
      "source": [
        "### Start training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjVoOggGWNuw"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GYZ8JqoYqZM"
      },
      "outputs": [],
      "source": [
        "# Fix for some CUDA bullshit\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "G46Zt8DdIexD",
        "outputId": "3ea8f605-de76-49d5-a24b-779e83603af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doing training on device type 'cuda'!\n",
            "BEGIN TRAINING!\n",
            "0\n",
            "Running batch:  0\n",
            " - Forward pass\n",
            " - Loss calc\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (100663296) must match the size of tensor b (6291456) at non-singleton dimension 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDoing training on device type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdevice_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_model(model, dataloader\u001b[39m=\u001b[39;49mdataloader, device\u001b[39m=\u001b[39;49mdevice, learning_rate\u001b[39m=\u001b[39;49mLEARNING_RATE, epochs\u001b[39m=\u001b[39;49mEPOCHS)\n",
            "\u001b[1;32m/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# preds = torch.argmax(outputs, dim=1).float()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m - Loss calc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m - Zero grad\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m intersection \u001b[39m=\u001b[39m (inputs \u001b[39m*\u001b[39;49m targets)\u001b[39m.\u001b[39msum()                            \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m dice \u001b[39m=\u001b[39m (\u001b[39m2.\u001b[39m\u001b[39m*\u001b[39mintersection \u001b[39m+\u001b[39m smooth)\u001b[39m/\u001b[39m(inputs\u001b[39m.\u001b[39msum() \u001b[39m+\u001b[39m targets\u001b[39m.\u001b[39msum() \u001b[39m+\u001b[39m smooth)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elal/Projects/almtech/aidetic/piper/training_data/research/feynman_2.ipynb#X63sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m dice\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100663296) must match the size of tensor b (6291456) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "print(f\"Doing training on device type '{device_type}'!\")\n",
        "torch.cuda.empty_cache()\n",
        "train_model(model, dataloader=dataloader, device=device, learning_rate=LEARNING_RATE, epochs=EPOCHS)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP9USO3QF69rEHBFKoGDxnI",
      "collapsed_sections": [
        "dx9KECZ6VY53",
        "OOR1CootWJqv"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
