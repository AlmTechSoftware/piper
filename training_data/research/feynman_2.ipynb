{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFWZzZp5Jj9ZS4mCeTC9zE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlmTechSoftware/piper/blob/main/training_data/research/feynman_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S9K_EnzFxBXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5198f32c-85fb-45ec-a189-0e722dd202a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4149k  100 4149k    0     0  2720k      0  0:00:01  0:00:01 --:--:-- 2719k\n",
            "Archive:  dataset.zip\n",
            " extracting: dataset/README.dataset.txt  \n",
            " extracting: dataset/README.roboflow.txt  \n",
            "   creating: dataset/test/\n",
            " extracting: dataset/test/Adam-still_jpeg.rf.53ade3c8a458fcd304c5a84d110e157c.jpg  \n",
            " extracting: dataset/test/Board_jpg.rf.9a7df51cf2743f861090462e582fd7ef.jpg  \n",
            " extracting: dataset/test/MPC_2_jpg.rf.0cd26e8fc0385c3e9c9bc14d7dcd8c94.jpg  \n",
            " extracting: dataset/test/_annotations.coco.json  \n",
            " extracting: dataset/test/falling_mp4-4_jpg.rf.7a0e2249c9ce2223be33120328072dab.jpg  \n",
            "   creating: dataset/train/\n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.27a169a78b8d38bc783fa79fb69d112b.jpg  \n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.70a11a4dac447e271f7d7af832c0d65f.jpg  \n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.aacb7831b459663607d7bf6b20d622fe.jpg  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.2c9a9cb02270f64fe388dc8b500182b7.jpg  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.665815102f6fc9631e74d2ebe5451385.jpg  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.8a4031e80c77df729f37196ef86294d0.jpg  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.30f72cef0bb93b6104b6b7396b70fd95.jpg  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.8df9b891c57153b63de804eb8617eca1.jpg  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.ffc25b51785b8bb23f3485844ab14ab9.jpg  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.5e4cf372bad80e274aea942ae5ab88fd.jpg  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.7bdce965f831828ab25b105ab8b3c4b6.jpg  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.c02653c106df75aba91b2f31fb05fd50.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.1a23679680b9140cf0b1ef18dde673cc.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.35c8ad16ecbe8b6c904d82a8eaf4b341.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.cbc1ba89cb8dbd8f7287650d30695f6a.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.0ee72c6f7e0e3925dd95d7e4d2ee9b17.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.8a7e8921aebbd532c4e0fda9758aae66.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.d597c32ec2e509ea53176121d6e5e227.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.3c7280ed1cc7f1d54e1c2e78b43b1474.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.add9450bb3f55e964b1e91bd908b0b8e.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.cd31631f2d3cebd9d1129b1b49e6bdfc.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.0695ac96837b3af9dd6ef35ee764c765.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.8dde705109bf7fbbb190e7a25d1cdeb9.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.f893195e6ca2eec64221368f9a1c4391.jpg  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.013f61874c45d4426534186a2a2cba51.jpg  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.8c05fa35619e64ace70e2e3d8400ceeb.jpg  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.bdc024d9d5094173736c0ba5083fd642.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.6633b960565dac3e8ad156ccbed8add4.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.a7f5c9724e97dbc4f5ae5d96c0b2b435.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.edf57187f5b68136f6fb14c9b5c9dfd4.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.0fd15588978f61b96a77cae6ef566053.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.85f3036c6478efaaf1501df5f820abbe.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.ae79302b6cc5a2776775cf435e9453b2.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.5c8fe4552d270eb6938a47032175e5ff.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.5eb4a4278ece7276cdb04d7a7662455e.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.e4e1fbc06d19ed500d4ca3ff40d680a9.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.1d0fc5d295eb8bc64e023f7fcddac421.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.86bbf01d1a6902d38baf54b6b96619e4.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.93db8c84984b1e7c07611ace44fee7fe.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.1388cabbe0fbd6b4678b49ee28061453.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.5ee1165fdf30989108198dba1f919088.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.9f4d43deef48e712baa5a37b81bc4f6e.jpg  \n",
            " extracting: dataset/train/_annotations.coco.json  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.398b424a9d961fd9ce2d19fe023f63ed.jpg  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.5b5b1fbc098ff5d2947510980dd61504.jpg  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.ec23df521a0fe650e6f57b5353f0500b.jpg  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.2dea44f666dcc2f53b59bf6052f0493b.jpg  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.537b4a3258eb56e8d4c49ec24725ee55.jpg  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.f771fd95a6c3c208eb738da53d493e91.jpg  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.70f5d131d4df4d1d875ac5216be3c1c5.jpg  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.89154ac8a8b3a849e284f405d817154d.jpg  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.b50418a348dd075c950cedcc7ccb4234.jpg  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.447f3a397411461aca622b660b39ded4.jpg  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.7254e15db57ce638542f308066b7f1c0.jpg  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.9b980d760a08ec1c1c392554b0616a2d.jpg  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.09ca2a8638318d01c2b60d4c412ee552.jpg  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.2949c4236c7988f248df22e4b91aaaa1.jpg  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.8664676927d5e1b38af43ff61d315460.jpg  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.8ad0fe5febcbb82a487d5dfcc4995dfb.jpg  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.9fa33eff605b0f63dee4f682a0846f1d.jpg  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.a8ab3fea9143f240c7f5f13ec667587c.jpg  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.203357a9fbe1fba12b0a5ac2c9b9eea6.jpg  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.282c22114780ba284af994f348bd717b.jpg  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.a58a91d799e4011dbf3c88e2f3f264a6.jpg  \n",
            " extracting: dataset/train/image_jpg.rf.1a7f536487655ce375381b4524d2ed4a.jpg  \n",
            " extracting: dataset/train/image_jpg.rf.93a94d7c3a1be17f1367eb803305de9d.jpg  \n",
            " extracting: dataset/train/image_jpg.rf.e2ed9f9e645c8cc6be2fc8895ce06ac2.jpg  \n",
            " extracting: dataset/train/ryerson2_png.rf.06619a9382ed59911168dd5a228aaab6.jpg  \n",
            " extracting: dataset/train/ryerson2_png.rf.6f910c234c2c3eff44623998f59a69f2.jpg  \n",
            " extracting: dataset/train/ryerson2_png.rf.de3db9ce8835d3497760a909aedb7246.jpg  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.214a41bb26f04cdac8a4d3b1501eeaa8.jpg  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.5e77a2970e4e942e958fc31fac5b5d23.jpg  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.7eeb278d304d9d15bd4a56592a360b05.jpg  \n",
            "   creating: dataset/valid/\n",
            " extracting: dataset/valid/20180910_153936_5b97bea9ddf2b358f90f884b_jpg.rf.539ebd2c0a594b6470b224f290aa64df.jpg  \n",
            " extracting: dataset/valid/D1D9CE52-103C-4BA1-A550-4F6F1018DC66_1_105_c-1_jpeg.rf.23e8c3bdf2f2dffd40fa9c9bef573b66.jpg  \n",
            " extracting: dataset/valid/EKF-UKF_jpg.rf.338425b6a9dc7c9451c767b20d45e620.jpg  \n",
            " extracting: dataset/valid/FqyewkjWwAE9ZsK_jpg.rf.26472ed32a5ca18332ef5b2506023c5b.jpg  \n",
            " extracting: dataset/valid/_annotations.coco.json  \n"
          ]
        }
      ],
      "source": [
        "!curl http://0x0.st/HLoi.zip > dataset.zip\n",
        "!unzip dataset.zip -d dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  opencv-python \\\n",
        "              scikit-image \\\n",
        "              python-dotenv \\\n",
        "              pycocotools \\\n",
        "              dataclasses-json \\\n",
        "              supervision \\\n",
        "              colored \\\n",
        "              wandb \\\n",
        "              torchvision -Uq"
      ],
      "metadata": {
        "id": "wrESAlTsxScr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0ff701-e244-40ad-a09f-64fd8455ccdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WandB login"
      ],
      "metadata": {
        "id": "9ENtKgHgx6RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "s5ANw45px5V5",
        "outputId": "ba96e407-19dc-472e-90b2-705d70996af4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Handler Definition"
      ],
      "metadata": {
        "id": "-OJlNPcXyxYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Tuple\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import pycocotools.coco as coco\n",
        "\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_dir: str,\n",
        "    ):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    # NOTE: ImageNet\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225],\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.coco_data = self.load_coco_data()\n",
        "\n",
        "    def load_coco_data(self):\n",
        "        coco_file_path = os.path.join(self.dataset_dir, \"_annotations.coco.json\")\n",
        "        return coco.COCO(coco_file_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coco_data.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = list(self.coco_data.imgs.values())[idx]\n",
        "        image_path = os.path.join(self.dataset_dir, image_info[\"file_name\"])\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Load and preprocess the segmentation mask\n",
        "        ann_ids = self.coco_data.getAnnIds(imgIds=image_info[\"id\"], iscrowd=None)\n",
        "        mask = np.zeros((image_info[\"height\"], image_info[\"width\"]))\n",
        "        for ann_id in ann_ids:\n",
        "            mask += self.coco_data.annToMask(self.coco_data.anns[ann_id])\n",
        "        mask = torch.tensor(mask).unsqueeze(0).float() / 255.0\n",
        "        mask = mask.repeat(3, 1, 1)\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "j1EmI8I7y0fX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "_tensor_pil_transform = T.ToPILImage()\n",
        "\n",
        "def tensor_to_pil(tensor: torch.Tensor):\n",
        "    return _tensor_pil_transform(tensor)"
      ],
      "metadata": {
        "id": "35hBUOfbzzvw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "PoCn7gmozJlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = COCODataset(\"dataset/train\")\n",
        "\n",
        "sak = dataset[0]\n",
        "display(sak)\n",
        "img, mask = sak\n",
        "img = tensor_to_pil(img)\n",
        "mask = tensor_to_pil(mask)\n",
        "display()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DSA6-SeXzI5c",
        "outputId": "050365d7-426a-4907-bbd1-f40ff2ffd2ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.8447,  0.9817,  1.0502,  ...,  2.0092,  2.0434,  2.0777],\n",
              "          [ 0.6906,  0.7762,  0.8104,  ...,  2.0263,  2.0777,  2.1119],\n",
              "          [ 0.5193,  0.5536,  0.5364,  ...,  2.0605,  2.0948,  2.1290],\n",
              "          ...,\n",
              "          [-0.5596, -0.4054, -0.3198,  ...,  1.2214,  1.4440,  1.9920],\n",
              "          [-0.5082, -0.4226, -0.3541,  ...,  0.9303,  1.2214,  1.8550],\n",
              "          [-0.5596, -0.5424, -0.4568,  ...,  0.6049,  0.9474,  1.6495]],\n",
              " \n",
              "         [[ 1.0805,  1.2206,  1.2906,  ...,  2.2010,  2.2360,  2.2710],\n",
              "          [ 0.9230,  1.0105,  1.0455,  ...,  2.2185,  2.2710,  2.3060],\n",
              "          [ 0.7479,  0.7829,  0.7654,  ...,  2.2535,  2.2885,  2.3235],\n",
              "          ...,\n",
              "          [-0.5651, -0.4076, -0.3550,  ...,  1.1155,  1.3431,  1.9034],\n",
              "          [-0.4601, -0.3725, -0.3550,  ...,  0.7654,  1.0630,  1.7108],\n",
              "          [-0.5126, -0.4951, -0.4601,  ...,  0.4328,  0.7829,  1.5007]],\n",
              " \n",
              "         [[ 1.4025,  1.5420,  1.6117,  ...,  2.5006,  2.5354,  2.5703],\n",
              "          [ 1.2457,  1.3328,  1.3677,  ...,  2.5180,  2.5703,  2.6051],\n",
              "          [ 1.0714,  1.1062,  1.0888,  ...,  2.5354,  2.5703,  2.6051],\n",
              "          ...,\n",
              "          [-0.4450, -0.2881, -0.2184,  ...,  1.2457,  1.4722,  2.0300],\n",
              "          [-0.3927, -0.3055, -0.2707,  ...,  0.9145,  1.2108,  1.8557],\n",
              "          [-0.4450, -0.4275, -0.3753,  ...,  0.5834,  0.9319,  1.6465]]]),\n",
              " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-259e5e273947>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msak\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_to_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msak\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-7e064741b28d>\u001b[0m in \u001b[0;36mtensor_to_pil\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensor_to_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_pil_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pic should be Tensor or ndarray. Got {type(pic)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'tuple'>."
          ]
        }
      ]
    }
  ]
}