{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "dx9KECZ6VY53",
        "OOR1CootWJqv"
      ],
      "authorship_tag": "ABX9TyPHZD7lMTbHC5UZsVPTJac1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlmTechSoftware/piper/blob/main/training_data/research/feynman_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "fOJFJG6WFQ09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset download from temporary 0x0.st"
      ],
      "metadata": {
        "id": "vCZ_EP0JChfu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S9K_EnzFxBXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5284de76-2979-498d-cd4c-1432a7768412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 12.1M  100 12.1M    0     0  8200k      0  0:00:01  0:00:01 --:--:-- 8205k\n",
            "rm: cannot remove 'dataset': No such file or directory\n",
            "Archive:  dataset.zip\n",
            " extracting: dataset/README.dataset.txt  \n",
            " extracting: dataset/README.roboflow.txt  \n",
            "   creating: dataset/test/\n",
            " extracting: dataset/test/Adam-still_jpeg.rf.90fbaea30ed3e09fd7fda5aa92ea6a68.jpg  \n",
            " extracting: dataset/test/Adam-still_jpeg.rf.90fbaea30ed3e09fd7fda5aa92ea6a68_mask.png  \n",
            " extracting: dataset/test/Board_jpg.rf.4d5d6a6f3c89eab87661fa09120877ef.jpg  \n",
            " extracting: dataset/test/Board_jpg.rf.4d5d6a6f3c89eab87661fa09120877ef_mask.png  \n",
            " extracting: dataset/test/MPC_2_jpg.rf.8263fb9cbb0b86857de4519a380ac872.jpg  \n",
            " extracting: dataset/test/MPC_2_jpg.rf.8263fb9cbb0b86857de4519a380ac872_mask.png  \n",
            " extracting: dataset/test/_classes.csv  \n",
            " extracting: dataset/test/falling_mp4-4_jpg.rf.6ae094b71ea51930490457b2e4125171.jpg  \n",
            " extracting: dataset/test/falling_mp4-4_jpg.rf.6ae094b71ea51930490457b2e4125171_mask.png  \n",
            "   creating: dataset/train/\n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.1c9ecbf5465654ef08acbcc9b7922bbd.jpg  \n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.1c9ecbf5465654ef08acbcc9b7922bbd_mask.png  \n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.23d95e4aeea4093dd35a5e5f7619cbeb.jpg  \n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.23d95e4aeea4093dd35a5e5f7619cbeb_mask.png  \n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.d9ed59739ac59ea149e4a82ff85337bd.jpg  \n",
            " extracting: dataset/train/15525772766_3abe50a63a_jpg.rf.d9ed59739ac59ea149e4a82ff85337bd_mask.png  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.32259bc83949539376620f43c63e1e94.jpg  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.32259bc83949539376620f43c63e1e94_mask.png  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.7b5add113ae513e95ab7ab733090a452.jpg  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.7b5add113ae513e95ab7ab733090a452_mask.png  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.ae06bb0b65634808ee7a1f51009a516f.jpg  \n",
            " extracting: dataset/train/20180910_132754_5b97bea52a6b222e6113ea32_jpg.rf.ae06bb0b65634808ee7a1f51009a516f_mask.png  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.2e68834b7758b5ccae93f75a98ce0393.jpg  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.2e68834b7758b5ccae93f75a98ce0393_mask.png  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.81c95d7e322110845d4db758c2099f4b.jpg  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.81c95d7e322110845d4db758c2099f4b_mask.png  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.b718b03863eb619c0f0dd69d2cd8b07b.jpg  \n",
            " extracting: dataset/train/Example-of-visualisation-on-whiteboard-This-visualisation-shows-the-design-of-the_png.rf.b718b03863eb619c0f0dd69d2cd8b07b_mask.png  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.cdbf9a742d3e8eb6692d595c7a78c2ab.jpg  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.cdbf9a742d3e8eb6692d595c7a78c2ab_mask.png  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.ceaa66a26b37dfd639ce1b53b68908f4.jpg  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.ceaa66a26b37dfd639ce1b53b68908f4_mask.png  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.eb863b41758dc3ad2ffef78669c68acc.jpg  \n",
            " extracting: dataset/train/MPC_1_jpg.rf.eb863b41758dc3ad2ffef78669c68acc_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.2effc684ec381df68a1c59444e6ae842.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.2effc684ec381df68a1c59444e6ae842_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.604953206fcaf4d49e9687789ade0917.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.604953206fcaf4d49e9687789ade0917_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.e27efdb2900101f66402821d9e08224c.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-0_jpg.rf.e27efdb2900101f66402821d9e08224c_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.02269d1f72b56e4a776eeaa5818a9c7e.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.02269d1f72b56e4a776eeaa5818a9c7e_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.b64b83e7ac581b01619008bf8e51d0dc.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.b64b83e7ac581b01619008bf8e51d0dc_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.ff21239d6783a8ef1f80838595445cef.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-1_jpg.rf.ff21239d6783a8ef1f80838595445cef_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.145d3469128c57577a7846a26cd8547a.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.145d3469128c57577a7846a26cd8547a_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.6445ddab4821b2b392a0fcdb86dcd522.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.6445ddab4821b2b392a0fcdb86dcd522_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.9cee5b07ac6de3b69f1af16ba52e5ff2.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-4_jpg.rf.9cee5b07ac6de3b69f1af16ba52e5ff2_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.31fb1aa74bc6f2c0e197c94fe614b227.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.31fb1aa74bc6f2c0e197c94fe614b227_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.9b31f1391fbbf326e8255c43bcf1dbd3.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.9b31f1391fbbf326e8255c43bcf1dbd3_mask.png  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.c7e1a106a6d183d9dae224562a7cd12a.jpg  \n",
            " extracting: dataset/train/Richard-Feynman-The_Character_of_Physical_Law-Part-2-full-version-kd0xTfdt6qw-_mp4-6_jpg.rf.c7e1a106a6d183d9dae224562a7cd12a_mask.png  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.04e2c8b48268aba06e99ad200cf32a9f.jpg  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.04e2c8b48268aba06e99ad200cf32a9f_mask.png  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.0b3f8ceef21cc580e722e98408f13dbe.jpg  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.0b3f8ceef21cc580e722e98408f13dbe_mask.png  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.589d8c1c2302bca76dd25a71fe860d6b.jpg  \n",
            " extracting: dataset/train/Screenshot_20230720_214513_png.rf.589d8c1c2302bca76dd25a71fe860d6b_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.53d93bdbb8bef0f32427467fc3f9bc38.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.53d93bdbb8bef0f32427467fc3f9bc38_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.866cf57787209adc713d27075754c5f5.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.866cf57787209adc713d27075754c5f5_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.aefa190eed8257e9c48be3fa439e0674.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_1_jpg.rf.aefa190eed8257e9c48be3fa439e0674_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.01d60a62b76f117f54e9728297453c80.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.01d60a62b76f117f54e9728297453c80_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.89dc195a65e328e12220c6a527ee3995.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.89dc195a65e328e12220c6a527ee3995_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.c83f94799cec4326d22650a12e341cfc.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_2_jpg.rf.c83f94799cec4326d22650a12e341cfc_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.2ece1c4245b8c490364d6146af3c4d94.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.2ece1c4245b8c490364d6146af3c4d94_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.4d0b5c39c6022d3d55db7262fc6d5568.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.4d0b5c39c6022d3d55db7262fc6d5568_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.5d4115a4c562f45d9be1578676f40e83.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_3_jpg.rf.5d4115a4c562f45d9be1578676f40e83_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.65a73e91aaecbf28db4c18fa8e298ae4.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.65a73e91aaecbf28db4c18fa8e298ae4_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.70cc1fc6d6da9481f82458419819a764.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.70cc1fc6d6da9481f82458419819a764_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.c5495732eb614363615366fe75431639.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_4_jpg.rf.c5495732eb614363615366fe75431639_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.15fbd62e5d9b20838ba4e842f0f2c528.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.15fbd62e5d9b20838ba4e842f0f2c528_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.6df86b259f435b2f5876fd8f867b8d50.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.6df86b259f435b2f5876fd8f867b8d50_mask.png  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.9d48580e5a573a1bb082a7e8f46cda54.jpg  \n",
            " extracting: dataset/train/Tracking_MPC_5_jpg.rf.9d48580e5a573a1bb082a7e8f46cda54_mask.png  \n",
            " extracting: dataset/train/_classes.csv  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.792126e4f073a9f50bb80350993df077.jpg  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.792126e4f073a9f50bb80350993df077_mask.png  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.98ef6fe43c13721cf9d9d5590f43f0a5.jpg  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.98ef6fe43c13721cf9d9d5590f43f0a5_mask.png  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.da0b6b300e747753632ace472e828a7d.jpg  \n",
            " extracting: dataset/train/falling_mp4-0_jpg.rf.da0b6b300e747753632ace472e828a7d_mask.png  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.73103eab2ebee697f220b689ee12581b.jpg  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.73103eab2ebee697f220b689ee12581b_mask.png  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.884575beeb997fa00bcbad4638042037.jpg  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.884575beeb997fa00bcbad4638042037_mask.png  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.bcc9e9c23749c9059d87804ee5cf71fb.jpg  \n",
            " extracting: dataset/train/falling_mp4-1_jpg.rf.bcc9e9c23749c9059d87804ee5cf71fb_mask.png  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.4e4a3007d9922bfee2f89c1f956c4e76.jpg  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.4e4a3007d9922bfee2f89c1f956c4e76_mask.png  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.c22cc8d253fa352f7f80290773728a36.jpg  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.c22cc8d253fa352f7f80290773728a36_mask.png  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.efabe64cb8a4855dbbc221a2b1c1b3ca.jpg  \n",
            " extracting: dataset/train/falling_mp4-2_jpg.rf.efabe64cb8a4855dbbc221a2b1c1b3ca_mask.png  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.2e6367a3689760d1f4a36055bb2244b9.jpg  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.2e6367a3689760d1f4a36055bb2244b9_mask.png  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.a6f9c8e5d3ca39c13c33e15da7a7bfc2.jpg  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.a6f9c8e5d3ca39c13c33e15da7a7bfc2_mask.png  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.f0f8561cd130e74e64ea9380f9a061f8.jpg  \n",
            " extracting: dataset/train/falling_mp4-3_jpg.rf.f0f8561cd130e74e64ea9380f9a061f8_mask.png  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.522554d11d09d61ce8f9ceacbd218b03.jpg  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.522554d11d09d61ce8f9ceacbd218b03_mask.png  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.861b7af8ec38daf9b4c011eaa31bc645.jpg  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.861b7af8ec38daf9b4c011eaa31bc645_mask.png  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.b901c618928b0af7fdfce7cbaef4beb1.jpg  \n",
            " extracting: dataset/train/falling_mp4-5_jpg.rf.b901c618928b0af7fdfce7cbaef4beb1_mask.png  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.189fe2ce9382521557cff4572a11f481.jpg  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.189fe2ce9382521557cff4572a11f481_mask.png  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.bd289e8292b4f7494786b3bdf7ad9906.jpg  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.bd289e8292b4f7494786b3bdf7ad9906_mask.png  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.d6b38d7329b0cfd99e172d493403df70.jpg  \n",
            " extracting: dataset/train/foto-9-schoolbord2OM_jpg.rf.d6b38d7329b0cfd99e172d493403df70_mask.png  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.4d3d6a88ff41d81b18508cb05cb16f07.jpg  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.4d3d6a88ff41d81b18508cb05cb16f07_mask.png  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.60063618604fe93c87b310dfabf8f0b4.jpg  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.60063618604fe93c87b310dfabf8f0b4_mask.png  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.922357268dc88b2ab5001fb5a7b20db7.jpg  \n",
            " extracting: dataset/train/foto6-formule-op-schoolbordOM_jpg.rf.922357268dc88b2ab5001fb5a7b20db7_mask.png  \n",
            " extracting: dataset/train/image_jpg.rf.0fd232bc743f74e9eda4b39914f834d2.jpg  \n",
            " extracting: dataset/train/image_jpg.rf.0fd232bc743f74e9eda4b39914f834d2_mask.png  \n",
            " extracting: dataset/train/image_jpg.rf.10c13c4e60c66cd0039417c997643f62.jpg  \n",
            " extracting: dataset/train/image_jpg.rf.10c13c4e60c66cd0039417c997643f62_mask.png  \n",
            " extracting: dataset/train/image_jpg.rf.9563a0544553516b083f85b1cbf2f360.jpg  \n",
            " extracting: dataset/train/image_jpg.rf.9563a0544553516b083f85b1cbf2f360_mask.png  \n",
            " extracting: dataset/train/ryerson2_png.rf.4f63dbede3705c2e08db4ea97b5e3312.jpg  \n",
            " extracting: dataset/train/ryerson2_png.rf.4f63dbede3705c2e08db4ea97b5e3312_mask.png  \n",
            " extracting: dataset/train/ryerson2_png.rf.71c0ca4380cac3c85c52b4c86abda6b1.jpg  \n",
            " extracting: dataset/train/ryerson2_png.rf.71c0ca4380cac3c85c52b4c86abda6b1_mask.png  \n",
            " extracting: dataset/train/ryerson2_png.rf.d5cbcbfb5c9698b2f662df29dd8fef9d.jpg  \n",
            " extracting: dataset/train/ryerson2_png.rf.d5cbcbfb5c9698b2f662df29dd8fef9d_mask.png  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.2099b56c443db2cfb4ee60e096608a0f.jpg  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.2099b56c443db2cfb4ee60e096608a0f_mask.png  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.a2a2ba3da6d9c65c8a5422ea562ea691.jpg  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.a2a2ba3da6d9c65c8a5422ea562ea691_mask.png  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.dc27f317c59df33238bef567ec60fd3f.jpg  \n",
            " extracting: dataset/train/whiteboard_jpg.rf.dc27f317c59df33238bef567ec60fd3f_mask.png  \n",
            "   creating: dataset/valid/\n",
            " extracting: dataset/valid/20180910_153936_5b97bea9ddf2b358f90f884b_jpg.rf.bd21c8dd22ab72203e447876f98b8031.jpg  \n",
            " extracting: dataset/valid/20180910_153936_5b97bea9ddf2b358f90f884b_jpg.rf.bd21c8dd22ab72203e447876f98b8031_mask.png  \n",
            " extracting: dataset/valid/D1D9CE52-103C-4BA1-A550-4F6F1018DC66_1_105_c-1_jpeg.rf.3ec235a322a99136a587c56039daa1db.jpg  \n",
            " extracting: dataset/valid/D1D9CE52-103C-4BA1-A550-4F6F1018DC66_1_105_c-1_jpeg.rf.3ec235a322a99136a587c56039daa1db_mask.png  \n",
            " extracting: dataset/valid/EKF-UKF_jpg.rf.8806f6a948a6c61b96ed9d830d813753.jpg  \n",
            " extracting: dataset/valid/EKF-UKF_jpg.rf.8806f6a948a6c61b96ed9d830d813753_mask.png  \n",
            " extracting: dataset/valid/FqyewkjWwAE9ZsK_jpg.rf.dd1a8ab6b2d03fc455b512e8a6be8caf.jpg  \n",
            " extracting: dataset/valid/FqyewkjWwAE9ZsK_jpg.rf.dd1a8ab6b2d03fc455b512e8a6be8caf_mask.png  \n",
            " extracting: dataset/valid/_classes.csv  \n"
          ]
        }
      ],
      "source": [
        "!curl http://0x0.st/HLtv.zip > dataset.zip\n",
        "!rm -r dataset\n",
        "!unzip dataset.zip -d dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "oqjcihTACqSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  opencv-python \\\n",
        "              scikit-image \\\n",
        "              python-dotenv \\\n",
        "              pycocotools \\\n",
        "              dataclasses-json \\\n",
        "              supervision \\\n",
        "              colored \\\n",
        "              wandb \\\n",
        "              torchvision \\\n",
        "              torchviz -Uq"
      ],
      "metadata": {
        "id": "wrESAlTsxScr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01529a5-51e9-4950-fa7e-f22a678d0c4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WandB login"
      ],
      "metadata": {
        "id": "9ENtKgHgx6RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5ANw45px5V5",
        "outputId": "1de70757-40e2-4af6-c2f9-6ad9d47014ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definitions"
      ],
      "metadata": {
        "id": "YDmM5s_1FcHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Handler Definition"
      ],
      "metadata": {
        "id": "-OJlNPcXyxYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "class PNGMaskDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_dir: str,\n",
        "    ):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Example _classes.csv\n",
        "        \"\"\"\n",
        "        Pixel Value, Class\n",
        "        0, background\n",
        "        1, sghug\n",
        "        2, w%\n",
        "        \"\"\"\n",
        "\n",
        "        # Parse _classes.csv\n",
        "        fh = open(os.path.join(self.dataset_dir, \"_classes.csv\"))\n",
        "        lines = fh.readlines()\n",
        "        fh.close()\n",
        "\n",
        "        self.classes = map(lambda csv: csv.split(\", \"), lines)\n",
        "        self.classes = map(lambda csv: (csv[1], int(csv[0])), self.classes)\n",
        "\n",
        "\n",
        "        # Load images & masks\n",
        "        self.mask_files = glob(os.path.join(self.dataset_dir, \"*_mask.png\"))\n",
        "\n",
        "        all_files =  glob(os.path.join(self.dataset_dir, \"*\"))\n",
        "        self.images = {}\n",
        "        for mask_file in self.mask_files:\n",
        "            img_file = mask_file.replace(\"_mask.png\", \".jpg\")\n",
        "            base, ext = os.path.splitext(img_file)\n",
        "\n",
        "            if img_file in all_files:\n",
        "                self.images[base] = {\n",
        "                    \"image\": img_file,\n",
        "                    \"mask\": mask_file,\n",
        "                }\n",
        "\n",
        "        self.tensors = {}\n",
        "        self.max_width = 0\n",
        "        self.max_height = 0\n",
        "        for base, image_info in self.images.items():\n",
        "            # Load and preprocess the image\n",
        "            image_path = image_info[\"image\"]\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            self.max_width = max(image.size[0], self.max_width)\n",
        "            image = self.transform(image)\n",
        "\n",
        "            # Load and preprocess the segmentation mask\n",
        "            mask_path = image_info[\"mask\"]\n",
        "            mask = Image.open(mask_path).convert(\"RGB\")\n",
        "            self.max_height = max(image.size[1], self.max_height)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "            self.tensors[base] = {\n",
        "                \"image\": image,\n",
        "                \"mask\": mask,\n",
        "            }\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_and_pad_tensor(input_tensor: Tensor,\n",
        "                            target_size: Tuple[int, int],\n",
        "                            padding_value: float = 0) -> Tensor:\n",
        "\n",
        "        input_size = input_tensor.size()\n",
        "        scale_factor = (target_size[0] / input_size[0], target_size[1] / input_size[1])\n",
        "\n",
        "        scaled_tensor = torch.nn.functional.interpolate(\n",
        "            input_tensor.unsqueeze(0),\n",
        "            scale_factor=scale_factor,\n",
        "            mode='bilinear',\n",
        "            align_corners=False,\n",
        "        ).squeeze(0)\n",
        "\n",
        "        pad_h = target_size[0] - scaled_tensor.size(1)\n",
        "        pad_w = target_size[1] - scaled_tensor.size(2)\n",
        "\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            padded_tensor = torch.nn.functional.pad(\n",
        "                scaled_tensor,\n",
        "                (0, pad_w, 0, pad_h),\n",
        "                value=padding_value,\n",
        "            )\n",
        "        else:\n",
        "            padded_tensor = scaled_tensor\n",
        "\n",
        "        return padded_tensor\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = list(self.tensors.values())[idx]\n",
        "        image, mask = image_info[\"image\"], image_info[\"mask\"]\n",
        "\n",
        "        image = self.scale_and_pad_tensor(image, (self.max_height, self.max_width))\n",
        "        mask = self.scale_and_pad_tensor(mask, (self.max_height, self.max_width))\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "j1EmI8I7y0fX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "_tensor_pil_transform = T.ToPILImage()\n",
        "\n",
        "def tensor_to_pil(tensor):\n",
        "    return _tensor_pil_transform(tensor)"
      ],
      "metadata": {
        "id": "35hBUOfbzzvw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "PoCn7gmozJlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PNGMaskDataset(\"dataset/train\")\n",
        "\n",
        "img, mask = dataset[0]\n",
        "\n",
        "img = tensor_to_pil(img)\n",
        "mask = tensor_to_pil(mask * 255)\n",
        "\n",
        "display(img)\n",
        "display(mask)\n",
        "\n",
        "del dataset, img, mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "DSA6-SeXzI5c",
        "outputId": "d64da26d-3cd4-4f0c-a441-a3543564145d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-516c86c75311>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPNGMaskDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_to_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-97ac0b20cc41>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_dir)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FeynMAN Model Definition"
      ],
      "metadata": {
        "id": "lCLnU8ZOCwlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import needed libs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Zy3bCK-GSRU5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "class FeynMAN(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, num_classes: int = 2):\n",
        "        super(__class__, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tensor:\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "K5bPDcMYC0Y9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Z8Cxaa7VFNbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi && echo \"YES YOU CAN TRAIN WITH CUDA!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af1ovG1ZFPmu",
        "outputId": "775a4adf-05be-4f1e-a7d7-c8b585cc3571"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 27 20:29:37 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "YES YOU CAN TRAIN WITH CUDA!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 8\n",
        "DATASET_DIR = \"dataset/train/\""
      ],
      "metadata": {
        "id": "gaRrGXg-F7CT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    dataset_dir: str = DATASET_DIR,\n",
        "    epochs: int = EPOCHS,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "    learning_rate: float = LEARNING_RATE,\n",
        "    device: torch.device = torch.device(\"cpu\"),\n",
        "):\n",
        "    # start a new wandb run to track this script\n",
        "    wandb.init(\n",
        "        # set the wandb project where this run will be logged\n",
        "        project=\"feynman_2\",\n",
        "\n",
        "        # track hyperparameters and run metadata\n",
        "        config={\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"architecture\": \"FeynMAN\",\n",
        "        \"dataset\": \"feynman_v7i\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Watch the model for cool weights & biases\n",
        "    wandb.watch(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() # nn.BCELoss() #### TODO: change?\n",
        "    params = model.parameters()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    dataset = PNGMaskDataset(dataset_dir)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(\"BEGIN TRAINING!\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        print(epoch)\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(dataloader):\n",
        "            images, masks = images.to(device), masks.to(device) #.float()\n",
        "            print(\"#\", batch_idx)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            print(\"post forward pass\")\n",
        "            loss = criterion(outputs, masks.squeeze(1))\n",
        "            print(\"loss done\")\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            print(\"zero grad done\")\n",
        "            loss.backward()\n",
        "            print(\"backward done\")\n",
        "            optimizer.step()\n",
        "            print(\"step done\")\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            wandb.log({\"Loss\": loss})\n",
        "\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{epochs}] Batch [{batch_idx+1}/{len(dataloader)}] - Loss: {loss:.4f}\"\n",
        "            )\n",
        "\n",
        "        wandb.log({\"Running loss\": running_loss})\n",
        "\n",
        "    print(\"END TRAINING!\")\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "MiG5glMSF3CF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model creation"
      ],
      "metadata": {
        "id": "fRs5JTpcWrnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FeynMAN()\n",
        "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#device_type = \"cpu\"\n",
        "device = torch.device(device_type)\n",
        "\n",
        "# Put the model on the GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "nQKN7f_HWtlO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Debug"
      ],
      "metadata": {
        "id": "dx9KECZ6VY53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the input etc.\n",
        "\n",
        "input_height = 256\n",
        "input_width = 256\n",
        "random_input = torch.randn(1, 3, input_height, input_width)\n",
        "\n",
        "# Forward pass\n",
        "segmentation_map = model(random_input)\n",
        "print(\"Segmentation map shape:\", segmentation_map.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGHH6MYVTXv0",
        "outputId": "16b80827-7100-43cd-baa4-634965f080d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation map shape: torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "ds = PNGMaskDataset(\"./dataset/test/\")\n",
        "x_tmp, _ = ds[0]\n",
        "x_tmp = x_tmp.to(device)\n",
        "y_tmp = model(x_tmp.unsqueeze(0))\n",
        "\n",
        "make_dot(y_tmp, params=dict(list(model.named_parameters()))).render(\"model.png\", format=\"png\")\n",
        "\n",
        "del model, ds, x_tmp, y_tmp, _"
      ],
      "metadata": {
        "id": "pCHT_PwG7hIX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 128, 128), batch_size=8, device=\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-zVTrnNT5IE",
        "outputId": "7614f362-46ba-4c7f-ee44-b02803238bf7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [8, 64, 128, 128]           1,792\n",
            "              ReLU-2          [8, 64, 128, 128]               0\n",
            "            Conv2d-3         [8, 128, 128, 128]          73,856\n",
            "              ReLU-4         [8, 128, 128, 128]               0\n",
            "            Conv2d-5         [8, 256, 128, 128]         295,168\n",
            "              ReLU-6         [8, 256, 128, 128]               0\n",
            "            Conv2d-7           [8, 3, 128, 128]             771\n",
            "================================================================\n",
            "Total params: 371,587\n",
            "Trainable params: 371,587\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.50\n",
            "Forward/backward pass size (MB): 899.00\n",
            "Params size (MB): 1.42\n",
            "Estimated Total Size (MB): 901.92\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start training loop"
      ],
      "metadata": {
        "id": "cBL0_8NEIbg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FCN ResNet 50 test"
      ],
      "metadata": {
        "id": "OOR1CootWJqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.segmentation import fcn_resnet50\n",
        "\n",
        "model = fcn_resnet50(num_classes=3, pretrained=False)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAszFGzqVrBU",
        "outputId": "0cb12e24-c1cd-43ef-8d00-ab56d07a6e1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "MjVoOggGWNuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix for some CUDA bullshit\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "0GYZ8JqoYqZM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Doing training on device type '{device_type}'!\")\n",
        "\n",
        "train_model(model, dataset_dir=\"./dataset/train/\", device=device, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "G46Zt8DdIexD",
        "outputId": "18baeb1a-2cb3-494b-f3c7-7cbbf38985d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwych\u001b[0m (\u001b[33malmtech\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doing training on device type 'cuda'!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230827_203108-f2rg1xp4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/almtech/feynman_2/runs/f2rg1xp4' target=\"_blank\">hopeful-waterfall-21</a></strong> to <a href='https://wandb.ai/almtech/feynman_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/almtech/feynman_2' target=\"_blank\">https://wandb.ai/almtech/feynman_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/almtech/feynman_2/runs/f2rg1xp4' target=\"_blank\">https://wandb.ai/almtech/feynman_2/runs/f2rg1xp4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEGIN TRAINING!\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d89a7f9ca097>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Doing training on device type '{device_type}'!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./dataset/train/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-2253b7faa3cd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset_dir, epochs, batch_size, learning_rate, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 360, 470] at entry 0 and [3, 1080, 1920] at entry 1"
          ]
        }
      ]
    }
  ]
}